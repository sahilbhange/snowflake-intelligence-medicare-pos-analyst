-- ============================================================================
-- AI Governance Objects: Table, View, and Stored Procedure
-- ============================================================================
-- Purpose: Extract AI agent governance metrics from Snowflake AI Observability
--
-- Problem: AI_OBSERVABILITY_EVENTS has 8+ rows per user question (different
--          span types) with nested JSON, making it hard to analyze.
--
-- Solution: Flatten multi-row spans into single governance records with all
--           metrics (tokens, cost, performance, SQL) in one queryable table.
--
-- Architecture:
--   AI_OBSERVABILITY_EVENTS (8+ rows)
--     → V_AI_GOVERNANCE_PARAMS (view: 1 row)
--     → AI_AGENT_GOVERNANCE (table: persistent storage)
-- ============================================================================

USE ROLE MEDICARE_POS_INTELLIGENCE;
USE DATABASE MEDICARE_POS_DB;
USE SCHEMA GOVERNANCE;

-- ============================================================================
-- 1. GOVERNANCE TABLE
-- ============================================================================
-- Stores flattened governance data for fast querying and dashboards
-- Each row = one user question/response with complete metrics
-- ============================================================================

CREATE TABLE IF NOT EXISTS GOVERNANCE.AI_AGENT_GOVERNANCE (
    -- Identity: Unique identifiers for tracking and deduplication
    request_id STRING PRIMARY KEY,           -- Unique ID for each user request
    trace_id STRING,                          -- OpenTelemetry trace ID (links all spans)
    user_name STRING,                         -- Snowflake user who asked the question
    role_name STRING,                         -- Role used during query
    thread_id NUMBER,                         -- Agent conversation thread ID

    -- Temporal: Time-based partitioning for efficient queries
    query_date DATE,                          -- Partition key for daily queries
    query_hour NUMBER,                        -- Hour of day (0-23) for hourly analysis

    -- Agent Configuration: What agent/model was used
    agent_name STRING,                        -- Name of the Cortex Agent
    agent_version NUMBER,                     -- Agent version number
    database_name STRING,                     -- Database where agent exists
    schema_name STRING,                       -- Schema where agent exists
    planning_model STRING,                    -- LLM model used (e.g., claude-haiku-4-5)

    -- User Interaction: Question and response
    user_question STRING,                     -- Original user question
    agent_response STRING,                    -- Agent's natural language response
    question_category STRING,                 -- Auto-categorized: ranking, aggregation, lookup, etc.

    -- Generated Artifacts: SQL and tool usage
    generated_sql STRING,                     -- SQL query generated by Cortex Analyst
    tools_used ARRAY,                         -- Array of tool names used (e.g., ['CortexAnalystTool', 'ChartTool'])
    tool_types ARRAY,                         -- Array of tool types (e.g., ['CortexAnalyst', 'ChartGeneration'])
    used_verified_query BOOLEAN,              -- Whether agent used a verified query from semantic model

    -- Performance Metrics: Latency and throughput
    total_duration_ms NUMBER,                 -- Total end-to-end execution time in milliseconds
    planning_duration_ms NUMBER,              -- LLM planning/reasoning time
    sql_generation_latency_ms NUMBER,         -- Time to generate SQL query
    performance_category STRING,              -- Auto-categorized: fast, moderate, slow, needs_optimization

    -- Token Metrics: LLM usage for cost tracking
    input_tokens NUMBER,                      -- Tokens sent to LLM (prompt + context)
    output_tokens NUMBER,                     -- Tokens generated by LLM (response)
    total_tokens NUMBER,                      -- Total tokens (input + output)
    cache_read_tokens NUMBER,                 -- Tokens retrieved from prompt cache
    cache_write_tokens NUMBER,                -- Tokens written to prompt cache
    cache_hit_rate_pct FLOAT,                 -- Cache efficiency (cache_read / input * 100)

    -- Cost Metrics: Financial tracking
    estimated_cost_usd FLOAT,                 -- Estimated cost based on token usage ($0.75/1M tokens)

    -- Quality Metrics: Success tracking
    execution_status STRING,                  -- Status: SUCCESS, ERROR, TIMEOUT
    status_code STRING,                       -- HTTP-style code: 200, 400, 500, etc.
    is_successful BOOLEAN,                    -- Quick filter for successful executions

    -- Audit: Timestamps for tracking and debugging
    start_timestamp TIMESTAMP,                -- When request started
    completion_timestamp TIMESTAMP,           -- When request completed
    created_at TIMESTAMP                      -- When this record was inserted
);

-- ============================================================================
-- 2. GOVERNANCE VIEW
-- ============================================================================
-- Flattens AI Observability events from 8+ rows per trace_id into 1 row
--
-- How AI Observability Stores Data:
--   Each user question generates multiple "spans" (OpenTelemetry concept):
--   - root_span: User question, response, status, duration
--   - planning_span: LLM model, tokens, cost, generated SQL
--   - tool_span(s): Tools executed (Cortex Analyst, Chart Generation, etc.)
--
-- This view JOINs these spans on trace_id to create one complete record
-- ============================================================================

CREATE OR REPLACE VIEW GOVERNANCE.V_AI_GOVERNANCE_PARAMS AS
WITH

-- ----------------------------------------------------------------------------
-- CTE 1: ROOT SPAN - User interaction data
-- ----------------------------------------------------------------------------
-- Filters for span_type = 'record_root' which contains:
--   - User's natural language question
--   - Agent's natural language response
--   - Execution status and timing
--   - Agent metadata (name, version, database)
-- ----------------------------------------------------------------------------
root_span AS (
    SELECT
        -- Trace ID: Unique identifier linking all spans for this request
        TRACE:trace_id::STRING AS trace_id,

        -- Timestamps: Start and end times
        TIMESTAMP AS completion_timestamp,
        START_TIMESTAMP,

        -- User Context: Who asked the question and with what role
        RESOURCE_ATTRIBUTES:"snow.user.name"::STRING AS user_name,
        RESOURCE_ATTRIBUTES:"snow.session.role.primary.name"::STRING AS role_name,

        -- Request Identity: Unique request ID for deduplication
        RECORD_ATTRIBUTES:"ai.observability.record_id"::STRING AS request_id,

        -- User Interaction: Question and response (from agent's final output)
        RECORD_ATTRIBUTES:"ai.observability.record_root.input"::STRING AS user_question,
        RECORD_ATTRIBUTES:"ai.observability.record_root.output"::STRING AS agent_response,

        -- Agent Context: Which agent, version, and conversation thread
        RECORD_ATTRIBUTES:"snow.ai.observability.agent.thread_id"::NUMBER AS thread_id,
        RECORD_ATTRIBUTES:"snow.ai.observability.object.name"::STRING AS agent_name,
        RECORD_ATTRIBUTES:"snow.ai.observability.object.version.id"::NUMBER AS agent_version,
        RECORD_ATTRIBUTES:"snow.ai.observability.database.name"::STRING AS database_name,
        RECORD_ATTRIBUTES:"snow.ai.observability.schema.name"::STRING AS schema_name,

        -- Performance: Total end-to-end execution time
        RECORD_ATTRIBUTES:"snow.ai.observability.agent.duration"::NUMBER AS total_duration_ms,

        -- Quality: Execution status and result code
        RECORD_ATTRIBUTES:"snow.ai.observability.agent.status"::STRING AS execution_status,
        RECORD_ATTRIBUTES:"snow.ai.observability.agent.status.code"::STRING AS status_code

    FROM SNOWFLAKE.LOCAL.AI_OBSERVABILITY_EVENTS
    WHERE RECORD_TYPE = 'SPAN'
      AND RECORD_ATTRIBUTES:"ai.observability.span_type"::STRING = 'record_root'
),

-- ----------------------------------------------------------------------------
-- CTE 2: PLANNING SPAN - LLM usage and SQL generation
-- ----------------------------------------------------------------------------
-- Filters for RECORD:name LIKE '%ResponseGeneration%' which contains:
--   - LLM model used (e.g., claude-haiku-4-5)
--   - Token counts (input, output, cache hits)
--   - Generated SQL query (stored as double-encoded JSON)
--   - SQL generation latency
-- ----------------------------------------------------------------------------
planning_span AS (
    SELECT
        TRACE:trace_id::STRING AS trace_id,

        -- LLM Configuration: Which model processed this request
        RECORD_ATTRIBUTES:"snow.ai.observability.agent.planning.model"::STRING AS planning_model,
        RECORD_ATTRIBUTES:"snow.ai.observability.agent.planning.duration"::NUMBER AS planning_duration_ms,

        -- Token Metrics: For cost calculation and optimization
        RECORD_ATTRIBUTES:"snow.ai.observability.agent.planning.token_count.input"::NUMBER AS input_tokens,
        RECORD_ATTRIBUTES:"snow.ai.observability.agent.planning.token_count.output"::NUMBER AS output_tokens,
        RECORD_ATTRIBUTES:"snow.ai.observability.agent.planning.token_count.total"::NUMBER AS total_tokens,
        RECORD_ATTRIBUTES:"snow.ai.observability.agent.planning.token_count.cache_read_input"::NUMBER AS cache_read_tokens,

        -- Generated SQL and Metadata
        -- IMPORTANT: tool_execution.results is DOUBLE-encoded JSON:
        --   1. Outer layer: JSON array as string: "[\"...\"]"
        --   2. Inner layer: Escaped JSON object: "{\"sql\":\"...\", \"analyst_latency_ms\":...}"
        -- Solution: Parse twice - first to get array element, second to parse that element

        TRY_PARSE_JSON(
            TRY_PARSE_JSON(
                RECORD_ATTRIBUTES:"snow.ai.observability.agent.planning.tool_execution.results"::STRING
            )[0]::STRING  -- First parse: get array, extract first element
        ):sql::STRING AS generated_sql,  -- Second parse: extract SQL from object

        TRY_PARSE_JSON(
            TRY_PARSE_JSON(
                RECORD_ATTRIBUTES:"snow.ai.observability.agent.planning.tool_execution.results"::STRING
            )[0]::STRING
        ):verified_query_used::BOOLEAN AS used_verified_query,  -- Whether query came from verified templates

        TRY_PARSE_JSON(
            TRY_PARSE_JSON(
                RECORD_ATTRIBUTES:"snow.ai.observability.agent.planning.tool_execution.results"::STRING
            )[0]::STRING
        ):analyst_latency_ms::NUMBER AS sql_generation_latency_ms  -- Time to generate SQL

    FROM SNOWFLAKE.LOCAL.AI_OBSERVABILITY_EVENTS
    WHERE RECORD_TYPE = 'SPAN'
      AND RECORD:name::STRING LIKE '%ResponseGeneration%'
),

-- ----------------------------------------------------------------------------
-- CTE 3: TOOL SPAN - Tool execution tracking
-- ----------------------------------------------------------------------------
-- Filters for RECORD:name LIKE '%Tool%' which contains:
--   - Names of tools executed (e.g., CortexAnalystTool, ChartGenerationTool)
--   - Tool types (categorized: CortexAnalyst, ChartGeneration, etc.)
--
-- Why ARRAY_AGG: One trace_id can use multiple tools, so we aggregate into arrays
-- Example: ["CortexAnalystTool_DMEPOS", "CortexChartToolImpl-data_to_chart"]
-- ----------------------------------------------------------------------------
tool_span AS (
    SELECT
        TRACE:trace_id::STRING AS trace_id,

        -- Aggregate all tool names used in this request into an array
        ARRAY_AGG(RECORD:name::STRING) AS tools_used,

        -- Categorize and aggregate tool types for easier analysis
        ARRAY_AGG(
            CASE
                WHEN RECORD:name::STRING LIKE '%CortexAnalyst%' THEN 'CortexAnalyst'
                WHEN RECORD:name::STRING LIKE '%Chart%' THEN 'ChartGeneration'
                ELSE SPLIT_PART(RECORD:name::STRING, '_', 1)  -- Extract first part as type
            END
        ) AS tool_types

    FROM SNOWFLAKE.LOCAL.AI_OBSERVABILITY_EVENTS
    WHERE RECORD_TYPE = 'SPAN'
      AND RECORD:name::STRING LIKE '%Tool%'
    GROUP BY TRACE:trace_id::STRING  -- One row per trace_id
)

-- ----------------------------------------------------------------------------
-- MAIN SELECT: Combine all CTEs into one row per trace_id
-- ----------------------------------------------------------------------------
-- LEFT JOINs ensure we get data even if some spans are missing
-- (e.g., if planning failed, we still want root span data)
-- ----------------------------------------------------------------------------
SELECT
    -- ========================================================================
    -- IDENTITY SECTION
    -- ========================================================================
    r.request_id,
    r.trace_id,
    r.user_name,
    r.role_name,
    r.thread_id,

    -- ========================================================================
    -- AGENT CONFIGURATION SECTION
    -- ========================================================================
    r.agent_name,
    r.agent_version,
    r.database_name,
    r.schema_name,
    p.planning_model,  -- From planning_span

    -- ========================================================================
    -- USER INTERACTION SECTION
    -- ========================================================================
    r.user_question,
    r.agent_response,

    -- Question Category: Auto-categorize based on keywords for analytics
    -- Helps identify query patterns: Are users asking for rankings? Aggregations?
    CASE
        WHEN LOWER(r.user_question) LIKE '%top%'
          OR LOWER(r.user_question) LIKE '%highest%'
          THEN 'ranking'
        WHEN LOWER(r.user_question) LIKE '%average%'
          OR LOWER(r.user_question) LIKE '%avg%'
          THEN 'aggregation'
        WHEN LOWER(r.user_question) LIKE '%what is%'
          OR LOWER(r.user_question) LIKE '%define%'
          THEN 'lookup'
        WHEN LOWER(r.user_question) LIKE '%compare%'
          THEN 'comparison'
        ELSE 'other'
    END AS question_category,

    -- ========================================================================
    -- GENERATED ARTIFACTS SECTION
    -- ========================================================================
    p.generated_sql,  -- From planning_span
    p.used_verified_query,  -- From planning_span

    -- Tool usage: COALESCE handles cases where no tools were used (returns empty array)
    COALESCE(t.tools_used, ARRAY_CONSTRUCT()) AS tools_used,  -- From tool_span
    COALESCE(t.tool_types, ARRAY_CONSTRUCT()) AS tool_types,  -- From tool_span

    -- ========================================================================
    -- PERFORMANCE SECTION
    -- ========================================================================
    r.total_duration_ms,
    p.planning_duration_ms,  -- From planning_span
    p.sql_generation_latency_ms,  -- From planning_span

    -- Performance Category: Auto-classify for quick filtering
    -- Thresholds based on user experience:
    --   < 2s   = fast (good UX)
    --   2-5s   = moderate (acceptable)
    --   5-10s  = slow (user may notice lag)
    --   > 10s  = needs optimization (poor UX)
    CASE
        WHEN r.total_duration_ms > 10000 THEN 'needs_optimization'
        WHEN r.total_duration_ms > 5000 THEN 'slow'
        WHEN r.total_duration_ms > 2000 THEN 'moderate'
        ELSE 'fast'
    END AS performance_category,

    -- ========================================================================
    -- TOKEN AND COST SECTION
    -- ========================================================================
    p.input_tokens,
    p.output_tokens,
    p.total_tokens,
    p.cache_read_tokens,

    -- Cache Hit Rate: Percentage of input tokens served from cache
    -- Higher = better (less LLM processing needed)
    ROUND(p.cache_read_tokens * 100.0 / NULLIF(p.input_tokens, 0), 1) AS cache_hit_rate_pct,

    -- Estimated Cost: Based on Claude pricing ($0.75 per 1M tokens)
    -- Formula: (total_tokens / 1,000,000) * $0.75
    -- Note: Actual pricing may vary by model and cache usage
    ROUND((p.total_tokens / 1000000.0) * 0.75, 4) AS estimated_cost_usd,

    -- ========================================================================
    -- QUALITY SECTION
    -- ========================================================================
    r.execution_status,  -- SUCCESS, ERROR, TIMEOUT, etc.
    r.status_code,       -- 200, 400, 500, etc.

    -- Boolean flag for easy filtering: WHERE is_successful = TRUE
    CASE
        WHEN r.execution_status = 'SUCCESS' AND r.status_code = '200' THEN TRUE
        ELSE FALSE
    END AS is_successful,

    -- ========================================================================
    -- TEMPORAL SECTION
    -- ========================================================================
    r.completion_timestamp,
    r.START_TIMESTAMP AS start_timestamp,

    -- Date and hour for partitioning and time-series analysis
    DATE(r.completion_timestamp) AS query_date,
    HOUR(r.completion_timestamp) AS query_hour,

    -- Record creation timestamp for audit trail
    CURRENT_TIMESTAMP() AS created_at

-- Join all CTEs on trace_id to create one complete row
FROM root_span r
LEFT JOIN planning_span p ON r.trace_id = p.trace_id
LEFT JOIN tool_span t ON r.trace_id = t.trace_id;


-- ============================================================================
-- 3. STORED PROCEDURE: Populate Governance Table
-- ============================================================================
-- Inserts new records from the view into the persistent table
--
-- Parameters:
--   LOOKBACK_DAYS: How many days back to extract (default 7)
--
-- Behavior:
--   - Queries V_AI_GOVERNANCE_PARAMS for recent data
--   - Skips records already in AI_AGENT_GOVERNANCE (deduplication)
--   - Returns count of new records inserted
--
-- Execute Rights: EXECUTE AS CALLER
--   Why: AI Observability access requires AI_OBSERVABILITY_EVENTS_LOOKUP
--        application role. With EXECUTE AS CALLER, the procedure uses the
--        caller's privileges (which includes this role), avoiding permission
--        errors. EXECUTE AS OWNER would fail because owner's privileges don't
--        inherit application roles properly.
-- ============================================================================

CREATE OR REPLACE PROCEDURE GOVERNANCE.POPULATE_AI_GOVERNANCE(
    LOOKBACK_DAYS NUMBER DEFAULT 7
)
RETURNS STRING
LANGUAGE SQL
EXECUTE AS CALLER  -- Critical: Uses caller's privileges to access AI Observability
AS
$$
BEGIN
    -- Insert new governance records from view into table
    INSERT INTO GOVERNANCE.AI_AGENT_GOVERNANCE
    SELECT
        request_id,
        trace_id,
        user_name,
        role_name,
        thread_id,
        query_date,
        query_hour,
        agent_name,
        agent_version,
        database_name,
        schema_name,
        planning_model,
        user_question,
        agent_response,
        question_category,
        generated_sql,
        tools_used,
        tool_types,
        used_verified_query,
        total_duration_ms,
        planning_duration_ms,
        sql_generation_latency_ms,
        performance_category,
        input_tokens,
        output_tokens,
        total_tokens,
        cache_read_tokens,
        0 AS cache_write_tokens,  -- Placeholder: cache writes not yet tracked
        cache_hit_rate_pct,
        estimated_cost_usd,
        execution_status,
        status_code,
        is_successful,
        start_timestamp,
        completion_timestamp,
        created_at
    FROM GOVERNANCE.V_AI_GOVERNANCE_PARAMS
    WHERE query_date >= DATEADD(day, -:LOOKBACK_DAYS, CURRENT_DATE())  -- Only recent data
      AND request_id NOT IN (  -- Deduplication: skip if already loaded
          SELECT request_id FROM GOVERNANCE.AI_AGENT_GOVERNANCE
      );

    -- Return success message with count of new records
    RETURN 'Successfully populated ' || SQLROWCOUNT || ' records';
END;
$$;


-- ============================================================================
-- VERIFICATION QUERIES
-- ============================================================================
-- Run these to verify the setup works correctly
-- ============================================================================

-- Test 1: Query the view for a specific trace_id
-- Should return 1 row with all fields populated (no NULLs in key columns)
SELECT * FROM GOVERNANCE.V_AI_GOVERNANCE_PARAMS
WHERE trace_id = '22a37156c76c6464b3825c514b3ffb27'
LIMIT 1;

-- Test 2: Run the stored procedure to populate the table
-- Should insert records for the last day
CALL GOVERNANCE.POPULATE_AI_GOVERNANCE(1);

-- Test 3: Verify data in governance table
-- Should show recently inserted records with complete metrics
SELECT
    query_date,
    user_name,
    question_category,
    performance_category,
    total_tokens,
    estimated_cost_usd,
    is_successful
FROM GOVERNANCE.AI_AGENT_GOVERNANCE
ORDER BY query_date DESC, completion_timestamp DESC
LIMIT 10;
